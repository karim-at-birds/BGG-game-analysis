{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential interesting Questions:\n",
    "- What are the games with either very good or very bad rating (high stddev)? Those might be the interesting ones. sort out more.\n",
    "\n",
    "- What does influence the rating?\n",
    "    - Does game length ~?\n",
    "    - Do prizes (honors column) ~?\n",
    "\n",
    "### rows dropped\n",
    "- if accessory rank is int, because I'm not interested in accessories\n",
    "\n",
    "### columns dropped  \n",
    "- version\n",
    "- compilation\n",
    "- accessory\n",
    "- implementation\n",
    "- expansion (replaced by expansion count)\n",
    "- thumbnail\n",
    "- commerce\n",
    "- honor (replaced by honor count)\n",
    "\n",
    "\n",
    "[Kaggle link](https://www.kaggle.com/datasets/seanthemalloy/board-game-geek-database/)\n",
    "\n",
    "[API description](https://boardgamegeek.com/wiki/page/BGG_XML_API2#toc3)\n",
    "\n",
    "## ToDo\n",
    "- Features reduzieren siehe aggregate_mlb_clean\n",
    "- Normalisierung der Linear Regression hinkriegen\n",
    "\n",
    "\n",
    "### Notes\n",
    "- too many publisher, family: Narrow down to the biggest 10 + stuff like kickstarter, web & selfpublished\n",
    "- sub-domain ok for one-hot encoding\n",
    "- category, mechanism  vielleicht zuviel für 1-hot\n",
    "- way to many artists and designer, biggest category uncredited -> reduce drastically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "file = os.path.join(data_folder, \"BGGdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(file):\n",
    "    bgg_df = pd.read_csv(file)\n",
    "else:\n",
    "    os.system(\"unzip \" + file + \".zip -d \" + data_folder)\n",
    "    bgg_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgg_df2 = bgg_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userrated_cutoff = 10 # line with less ratings get deleted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## deleting rows containing accessories, not games as well as row without too few ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgg_df2 = bgg_df2[bgg_df2[\"accessoryrank\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgg_df2 = bgg_df2[bgg_df2[\"usersrated\"] >= userrated_cutoff]\n",
    "#bgg_df2.sort_values([\"usersrated\"], ascending=False).filter(like=\"name\").head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## converting some categories to counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Honor count is more interesting than which honors\n",
    "bgg_df2[\"hon_count\"] = bgg_df2.filter(like=\"honor\").count(axis=1)\n",
    "# expansion count is more interesting than which expansions\n",
    "bgg_df2[\"expan_count\"] = bgg_df.filter(like=\"expansion\").count(axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropping uninteresting columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df:pd.DataFrame, search_string:list = None)-> None:\n",
    "    '''\n",
    "    Without search_string function prompts for \n",
    "    With search_string deleted columns whose title includes the search_string\n",
    "    '''\n",
    "    if search_string:\n",
    "        search_string =  search_string if type(search_string) == str else \"(?i)\"+\"|\".join(search_string)\n",
    "        df_temp = df.filter(regex=search_string)\n",
    "        drop_column = df_temp.columns\n",
    "        df.drop(columns=drop_column, inplace = True)\n",
    "        return None\n",
    "\n",
    "    search_string = input(\"filter string:\")\n",
    "    df_temp = df.filter(like=search_string)\n",
    "    drop_column = df_temp.columns #if regex else df.filter(like=search_string).columns\n",
    "    display(drop_column)\n",
    "    \n",
    "    answer = input(\"drop? yes/display/exit/\")\n",
    "\n",
    "    if answer == \"yes\":\n",
    "        df.drop(columns=drop_column, inplace = True)\n",
    "        drop_columns(df)\n",
    "    elif answer == \"display\":\n",
    "        display(df[drop_column].dropna().head())\n",
    "        drop_columns(df)\n",
    "    elif answer == \"exit\":\n",
    "        return None\n",
    "    else:\n",
    "        drop_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping uninteresting columns\n",
    "drop_columns(bgg_df2, [\"version\",\"compilation\",\"accessory\", \"implementation\", \"expansion\", \"thumbnail\",\"commerce\",\"honor\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical variables\n",
    "## exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_value_counts(df:pd.DataFrame, columns:str):\n",
    "    uniques = set()\n",
    "    temp_df = pd.DataFrame()\n",
    "    for column in df.filter(like=columns):\n",
    "        uniques = uniques.union(set(df[column].unique()))\n",
    "        temp_df[column] = df[column].value_counts()\n",
    "    df_out = temp_df.sum(axis=1).sort_values(ascending=False)\n",
    "    return df_out, uniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "honor_counts = sum_value_counts(bgg_df.drop(columns='familyrank'), \"category\")[0]\n",
    "honor_counts.sort_index(ascending=False)\n",
    "honor_counts.sort_values(ascending=True).sort_values(ascending=False).head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reducing categories\n",
    "\n",
    "1. misc category\n",
    "1. aggregating similar categories *seems like a lot of work, let's try if misc is enough * \n",
    "    1. mech\n",
    "        1. worker placement\n",
    "        1. auction\n",
    "    1. categ\n",
    "        1. america wars\n",
    "    1. honor\n",
    "        1. count/delete nominations * a LOT of work*\n",
    "        1. count awards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating dummy variables\n",
    "1. mechanic\n",
    "1. subdomain\n",
    "1. family\n",
    "1. category\n",
    "1. publisher\n",
    "1. artist\n",
    "1. designer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_one_hot_and_aggregate(df:pd.DataFrame, cutoff_type:str, param:int, agg:bool) -> pd.DataFrame:\n",
    "    bools = None\n",
    "    if cutoff_type == \"min_freq\":\n",
    "        bools = (df.sum() < param).values\n",
    "        df_out1 = df[df.columns[~bools]]\n",
    "    agg_array = df[df.columns[bools]].sum(axis=1) if agg else None\n",
    "    return df_out1, agg_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_mlb_clean(df:pd.DataFrame, column_search_str:str, misc:tuple[str,int,bool] = (\"min_freq\",1000,False)) -> tuple[pd.DataFrame,MultiLabelBinarizer,np.ndarray]:\n",
    "    '''\n",
    "    aggregate several columns of the same multi-label variable in one column as lists, one hot encode it and deleted superfluous columns\n",
    "\n",
    "    df: input dataframe\n",
    "    misc: Da muss ich mir noch überlegen welche Arten ich will. Top x? x% ? Cutoff wenn zuviele?\n",
    "    '''\n",
    "    df[column_search_str] = df.filter(regex= column_search_str + \"[0-9]\").fillna(\"NaN\").values.tolist()\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb_array = mlb.fit_transform(df[column_search_str])\n",
    "    drop_columns(df, column_search_str)\n",
    "    df2 = pd.DataFrame(mlb_array, columns= column_search_str + \"_\" + mlb.classes_, index=df.index).drop(columns=column_search_str + \"_\" +\"NaN\")\n",
    "    df2, agg_array = drop_one_hot_and_aggregate(df2, *misc)\n",
    "    if agg_array is not None:\n",
    "        df2[column_search_str + \"_misc\"] = agg_array\n",
    "    df_out = df.join(df2)\n",
    "    return df_out, mlb, mlb_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karim2, karim_mlb,karim_mlb_array = aggregate_mlb_clean(bgg_df2.copy(), \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(karim2.columns))\n",
    "karim2, _ , _ = aggregate_mlb_clean(karim2, \"mechanic\")\n",
    "print(len(karim2.columns))\n",
    "karim2, _ , _ = aggregate_mlb_clean(karim2, \"subdomain\")\n",
    "print(len(karim2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karim2, _, _ = aggregate_mlb_clean(karim2, \"family\")\n",
    "print(len(karim2.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karim2, _, _ = aggregate_mlb_clean(karim2, \"publisher\")\n",
    "print(len(karim2.columns))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def clean_fit_linear_mod(df, response_col, cat_cols, dummy_na, test_size=.3, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column \n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test \n",
    "    \n",
    "    OUTPUT:\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    '''\n",
    "    #Drop the rows with missing response values\n",
    "    df  = df.dropna(subset=[response_col], axis=0)\n",
    "\n",
    "    #Drop columns with all NaN values\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "\n",
    "    #Dummy categorical variables\n",
    "    #df = create_dummy_df(df, cat_cols, dummy_na)\n",
    "\n",
    "    # Mean function\n",
    "    fill_mean = lambda col: col.fillna(col.mean())\n",
    "    # Fill the mean\n",
    "    df = df.apply(fill_mean, axis=0)\n",
    "\n",
    "    #Split into explanatory and response variables\n",
    "    X = df.drop(response_col, axis=1)\n",
    "    y = df[response_col]\n",
    "\n",
    "    #Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n",
    "\n",
    "    # krieg das normalisieren (stichwort skaling?) nicht hin \n",
    "    #min_max_scaler = MinMaxScaler().fit(X_test)\n",
    "    #X_norm = min_max_scaler.transform(X) # Keine ahnung was das macht\n",
    "    #  \n",
    "    lm_model = LinearRegression() # Instantiate\n",
    "    lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "    #Predict using your model\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "    #Score using your model\n",
    "    test_score = r2_score(y_test, y_test_preds)\n",
    "    train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "#Test your function with the above dataset\n",
    "cat_cols_lst = None\n",
    "test_score, train_score, lm_model, X_train, X_test, y_train, y_test = clean_fit_linear_mod(karim2.select_dtypes(exclude=\"object\"), 'average', cat_cols_lst, dummy_na=False)\n",
    "test_score, train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das mal ausprobieren und Ergebnisse anschauen https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3299249671443033, 0.3350201909325803\n",
    "\n",
    "def coef_weights(coefficients, X_train):\n",
    "    '''\n",
    "    INPUT:\n",
    "    coefficients - the coefficients of the linear model \n",
    "    X_train - the training data, so the column names can be used\n",
    "    OUTPUT:\n",
    "    coefs_df - a dataframe holding the coefficient, estimate, and abs(estimate)\n",
    "    \n",
    "    Provides a dataframe that can be used to understand the most influential coefficients\n",
    "    in a linear model by providing the coefficient estimates along with the name of the \n",
    "    variable attached to the coefficient.\n",
    "    '''\n",
    "    coefs_df = pd.DataFrame()\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    coefs_df['coefs'] = lm_model.coef_\n",
    "    coefs_df['abs_coefs'] = np.abs(lm_model.coef_)\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "    return coefs_df\n",
    "\n",
    "#Use the function\n",
    "coef_df = coef_weights(lm_model.coef_, X_train)\n",
    "\n",
    "#A quick look at the top results\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model.intercept_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for divisive games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgg_df.sort_values(by=\"usersrated\", ascending=False).head(500) #\n",
    "bgg_df.sort_values(by=\"stddev\", ascending=True)[bgg_df[\"usersrated\"] > 100 ]\n",
    "\n",
    "#Bins machen für average und mir die mittlere anschauen und stddev nach oben sortieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgg_df.iloc[:,:10].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(bgg_df.corr().iloc[:10,:10], annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgg_df.corr().iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a1808185fcc1dd3412e25e7f1e2f1855e9e63afcabf27c30a1072d050815e5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
